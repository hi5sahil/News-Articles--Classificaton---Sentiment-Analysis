{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Library and setting environment path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# set the path \n",
    "\n",
    "sparkPath = \"C:/Users/mishr/Downloads/spark-2.0.2-bin-hadoop2.7/spark-2.0.2-bin-hadoop2.7/spark-2.0.2-bin-hadoop2.7\"\n",
    "os.environ['SPARK_HOME'] = sparkPath\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.4.0 pyspark-shell'\n",
    "sys.path.append(sparkPath + '/bin')\n",
    "sys.path.append(sparkPath + '/python')\n",
    "sys.path.append(sparkPath + '/python/pyspark')\n",
    "sys.path.append(sparkPath + '/python/pyspark/lib')\n",
    "sys.path.append(sparkPath + '/python/pyspark/lib/pyspark.zip')\n",
    "sys.path.append(sparkPath + '/python/pyspark/lib/py4j-0.10.3-src.zip')\n",
    "sys.path.append(\"C:/Program Files (x86)/Java/jre1.8.0_111/bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Reading the data from newCorpora.csv which is a tab separated file and reading it to originalRDD\n",
    "originalRDD=sc.textFile(\"file:///E:/Ananya/MSBA_Carlson/Fall/Big Data/Project/newsCorpora.csv\").map(lambda line: line.split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'Fed official says weak data caused by weather, should not slow taper',\n",
       " 'http://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\\\?track=rss',\n",
       " 'Los Angeles Times',\n",
       " 'b',\n",
       " 'ddUyU0VZz0BRneMioxUPQVP6sIxvM',\n",
       " 'www.latimes.com',\n",
       " '1394470370698']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking first row of the originalRDD to have a view of its structure\n",
    "originalRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We select the date and title column from the originalRDD and read it to a new RDD called rowRDD\n",
    "rowRDD = originalRDD.map(lambda p: Row(title=p[1], label=p[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label='b', title='Fed official says weak data caused by weather, should not slow taper')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking first row of the rowRDD to have a view of its structure\n",
    "rowRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We then filter out the news based on the various categories-business,entertainment,technology and medicine\n",
    "business = rowRDD.filter(lambda line: line[0] == 'b')\n",
    "entertainment = rowRDD.filter(lambda line: line[0] == 'e')\n",
    "technology = rowRDD.filter(lambda line: line[0] == 't')\n",
    "medicine = rowRDD.filter(lambda line: line[0] == 'm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the purpose of classification we convert the category column to ordinal data format\n",
    "businessLabel = business.map(lambda line: (0, line[1]))\n",
    "entertainmentLabel = entertainment.map(lambda line: (1, line[1]))\n",
    "technologyLabel = technology.map(lambda line: (3, line[1]))\n",
    "medicineLabel = medicine.map(lambda line: (4, line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'Fed official says weak data caused by weather, should not slow taper')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We then combined all the different categories into a single RDD to analyse them further\n",
    "newsAllLabel = sc.union([businessLabel, entertainmentLabel, technologyLabel, medicineLabel])\n",
    "newsAllLabel.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We set the second column as title and the category column as label\n",
    "newsAllLabelROW = newsAllLabel.map(lambda p: Row(title=p[1], label=p[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing SQLContext\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Converting newsAllLabelRow into dataframe and then registering it as a temporary table\n",
    "news = sqlContext.createDataFrame(newsAllLabelROW)\n",
    "news.registerTempTable(\"news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Setting the labels for the dataframe\n",
    "news = news.selectExpr(\"cast(label as double) as label\", \"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Viewing the schema structure of the news table\n",
    "news.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|               title|        review_words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Fed official says...|[fed, official, s...|\n",
      "|  0.0|Fed's Charles Plo...|[fed's, charles, ...|\n",
      "|  0.0|US open: Stocks f...|[us, open:, stock...|\n",
      "|  0.0|Fed risks falling...|[fed, risks, fall...|\n",
      "|  0.0|Fed's Plosser: Na...|[fed's, plosser:,...|\n",
      "|  0.0|Plosser: Fed May ...|[plosser:, fed, m...|\n",
      "|  0.0|Fed's Plosser: Ta...|[fed's, plosser:,...|\n",
      "|  0.0|Fed's Plosser exp...|[fed's, plosser, ...|\n",
      "|  0.0|US jobs growth la...|[us, jobs, growth...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|\n",
      "|  0.0|EU's half-baked b...|[eu's, half-baked...|\n",
      "|  0.0|Europe reaches cr...|[europe, reaches,...|\n",
      "|  0.0|ECB FOCUS-Stronge...|[ecb, focus-stron...|\n",
      "|  0.0|EU aims for deal ...|[eu, aims, for, d...|\n",
      "|  0.0|Forex - Pound dro...|[forex, -, pound,...|\n",
      "|  0.0|Noyer Says Strong...|[noyer, says, str...|\n",
      "|  0.0|EU Week Ahead Mar...|[eu, week, ahead,...|\n",
      "|  0.0|ECB member Noyer ...|[ecb, member, noy...|\n",
      "|  0.0|Euro Anxieties Wa...|[euro, anxieties,...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 1:Tokenize the review. \n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"review_words\")\n",
    "reviewDF = tokenizer.transform(news)\n",
    "reviewDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|               title|        review_words|            filtered|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Fed official says...|[fed, official, s...|[fed, official, s...|\n",
      "|  0.0|Fed's Charles Plo...|[fed's, charles, ...|[fed's, charles, ...|\n",
      "|  0.0|US open: Stocks f...|[us, open:, stock...|[us, open:, stock...|\n",
      "|  0.0|Fed risks falling...|[fed, risks, fall...|[fed, risks, fall...|\n",
      "|  0.0|Fed's Plosser: Na...|[fed's, plosser:,...|[fed's, plosser:,...|\n",
      "|  0.0|Plosser: Fed May ...|[plosser:, fed, m...|[plosser:, fed, m...|\n",
      "|  0.0|Fed's Plosser: Ta...|[fed's, plosser:,...|[fed's, plosser:,...|\n",
      "|  0.0|Fed's Plosser exp...|[fed's, plosser, ...|[fed's, plosser, ...|\n",
      "|  0.0|US jobs growth la...|[us, jobs, growth...|[us, jobs, growth...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|\n",
      "|  0.0|EU's half-baked b...|[eu's, half-baked...|[eu's, half-baked...|\n",
      "|  0.0|Europe reaches cr...|[europe, reaches,...|[europe, reaches,...|\n",
      "|  0.0|ECB FOCUS-Stronge...|[ecb, focus-stron...|[ecb, focus-stron...|\n",
      "|  0.0|EU aims for deal ...|[eu, aims, for, d...|[eu, aims, deal, ...|\n",
      "|  0.0|Forex - Pound dro...|[forex, -, pound,...|[forex, -, pound,...|\n",
      "|  0.0|Noyer Says Strong...|[noyer, says, str...|[noyer, says, str...|\n",
      "|  0.0|EU Week Ahead Mar...|[eu, week, ahead,...|[eu, week, ahead,...|\n",
      "|  0.0|ECB member Noyer ...|[ecb, member, noy...|[ecb, member, noy...|\n",
      "|  0.0|Euro Anxieties Wa...|[euro, anxieties,...|[euro, anxieties,...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 2:Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"review_words\", outputCol=\"filtered\")\n",
    "filteredDF = remover.transform(reviewDF)\n",
    "filteredDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               title|        review_words|            filtered|                  TF|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Fed official says...|[fed, official, s...|[fed, official, s...|(262144,[4900,769...|\n",
      "|  0.0|Fed's Charles Plo...|[fed's, charles, ...|[fed's, charles, ...|(262144,[1889,306...|\n",
      "|  0.0|US open: Stocks f...|[us, open:, stock...|[us, open:, stock...|(262144,[7695,119...|\n",
      "|  0.0|Fed risks falling...|[fed, risks, fall...|[fed, risks, fall...|(262144,[7695,583...|\n",
      "|  0.0|Fed's Plosser: Na...|[fed's, plosser:,...|[fed's, plosser:,...|(262144,[30249,66...|\n",
      "|  0.0|Plosser: Fed May ...|[plosser:, fed, m...|[plosser:, fed, m...|(262144,[1889,769...|\n",
      "|  0.0|Fed's Plosser: Ta...|[fed's, plosser:,...|[fed's, plosser:,...|(262144,[1889,277...|\n",
      "|  0.0|Fed's Plosser exp...|[fed's, plosser, ...|[fed's, plosser, ...|(262144,[21872,37...|\n",
      "|  0.0|US jobs growth la...|[us, jobs, growth...|[us, jobs, growth...|(262144,[5381,218...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|(262144,[45531,72...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|(262144,[17154,72...|\n",
      "|  0.0|EU's half-baked b...|[eu's, half-baked...|[eu's, half-baked...|(262144,[4842,343...|\n",
      "|  0.0|Europe reaches cr...|[europe, reaches,...|[europe, reaches,...|(262144,[4842,292...|\n",
      "|  0.0|ECB FOCUS-Stronge...|[ecb, focus-stron...|[ecb, focus-stron...|(262144,[15291,29...|\n",
      "|  0.0|EU aims for deal ...|[eu, aims, for, d...|[eu, aims, deal, ...|(262144,[20661,93...|\n",
      "|  0.0|Forex - Pound dro...|[forex, -, pound,...|[forex, -, pound,...|(262144,[27352,45...|\n",
      "|  0.0|Noyer Says Strong...|[noyer, says, str...|[noyer, says, str...|(262144,[9509,506...|\n",
      "|  0.0|EU Week Ahead Mar...|[eu, week, ahead,...|[eu, week, ahead,...|(262144,[15516,93...|\n",
      "|  0.0|ECB member Noyer ...|[ecb, member, noy...|[ecb, member, noy...|(262144,[9509,235...|\n",
      "|  0.0|Euro Anxieties Wa...|[euro, anxieties,...|[euro, anxieties,...|(262144,[44992,50...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Row(TF=SparseVector(262144, {4900: 1.0, 7695: 1.0, 27707: 1.0, 44133: 1.0, 70520: 1.0, 97162: 1.0, 151894: 1.0, 160735: 1.0, 161826: 1.0}), label=0.0)\n",
      "Row(TF=SparseVector(262144, {1889: 1.0, 3067: 1.0, 81535: 1.0, 92646: 1.0, 106453: 1.0, 108453: 1.0, 136020: 1.0, 175637: 1.0, 235090: 1.0}), label=0.0)\n",
      "Row(TF=SparseVector(262144, {7695: 1.0, 11910: 1.0, 21872: 1.0, 37521: 1.0, 44133: 1.0, 58267: 1.0, 141063: 1.0, 235090: 1.0, 255396: 1.0}), label=0.0)\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|               title|        review_words|            filtered|                  TF|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Fed official says...|[fed, official, s...|[fed, official, s...|(262144,[4900,769...|(262144,[4900,769...|\n",
      "|  0.0|Fed's Charles Plo...|[fed's, charles, ...|[fed's, charles, ...|(262144,[1889,306...|(262144,[1889,306...|\n",
      "|  0.0|US open: Stocks f...|[us, open:, stock...|[us, open:, stock...|(262144,[7695,119...|(262144,[7695,119...|\n",
      "|  0.0|Fed risks falling...|[fed, risks, fall...|[fed, risks, fall...|(262144,[7695,583...|(262144,[7695,583...|\n",
      "|  0.0|Fed's Plosser: Na...|[fed's, plosser:,...|[fed's, plosser:,...|(262144,[30249,66...|(262144,[30249,66...|\n",
      "|  0.0|Plosser: Fed May ...|[plosser:, fed, m...|[plosser:, fed, m...|(262144,[1889,769...|(262144,[1889,769...|\n",
      "|  0.0|Fed's Plosser: Ta...|[fed's, plosser:,...|[fed's, plosser:,...|(262144,[1889,277...|(262144,[1889,277...|\n",
      "|  0.0|Fed's Plosser exp...|[fed's, plosser, ...|[fed's, plosser, ...|(262144,[21872,37...|(262144,[21872,37...|\n",
      "|  0.0|US jobs growth la...|[us, jobs, growth...|[us, jobs, growth...|(262144,[5381,218...|(262144,[5381,218...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|(262144,[45531,72...|(262144,[45531,72...|\n",
      "|  0.0|ECB unlikely to e...|[ecb, unlikely, t...|[ecb, unlikely, e...|(262144,[17154,72...|(262144,[17154,72...|\n",
      "|  0.0|EU's half-baked b...|[eu's, half-baked...|[eu's, half-baked...|(262144,[4842,343...|(262144,[4842,343...|\n",
      "|  0.0|Europe reaches cr...|[europe, reaches,...|[europe, reaches,...|(262144,[4842,292...|(262144,[4842,292...|\n",
      "|  0.0|ECB FOCUS-Stronge...|[ecb, focus-stron...|[ecb, focus-stron...|(262144,[15291,29...|(262144,[15291,29...|\n",
      "|  0.0|EU aims for deal ...|[eu, aims, for, d...|[eu, aims, deal, ...|(262144,[20661,93...|(262144,[20661,93...|\n",
      "|  0.0|Forex - Pound dro...|[forex, -, pound,...|[forex, -, pound,...|(262144,[27352,45...|(262144,[27352,45...|\n",
      "|  0.0|Noyer Says Strong...|[noyer, says, str...|[noyer, says, str...|(262144,[9509,506...|(262144,[9509,506...|\n",
      "|  0.0|EU Week Ahead Mar...|[eu, week, ahead,...|[eu, week, ahead,...|(262144,[15516,93...|(262144,[15516,93...|\n",
      "|  0.0|ECB member Noyer ...|[ecb, member, noy...|[ecb, member, noy...|(262144,[9509,235...|(262144,[9509,235...|\n",
      "|  0.0|Euro Anxieties Wa...|[euro, anxieties,...|[euro, anxieties,...|(262144,[44992,50...|(262144,[44992,50...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 3:Convert to TF words vector\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"TF\")\n",
    "TFDF = hashingTF.transform(filteredDF)\n",
    "TFDF.show()\n",
    "## HashingTF in SparkML cannot normalize term frequency with the total number of words in each document\n",
    "for features_label in TFDF.select(\"TF\", \"label\").take(3):\n",
    "    print(features_label)\n",
    "\n",
    "# Convert to IDF words vector, ensure to name the features as 'features'\n",
    "idf = IDF(inputCol=\"TF\", outputCol=\"features\")\n",
    "idfModel = idf.fit(TFDF)\n",
    "finalDF = idfModel.transform(TFDF)\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0), Row(label=1.0), Row(label=4.0), Row(label=3.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalDF.select('label').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=SparseVector(262144, {4900: 6.5481, 7695: 5.3318, 27707: 6.8659, 44133: 6.1187, 70520: 8.966, 97162: 6.8259, 151894: 9.0632, 160735: 4.7292, 161826: 3.8551}), label=0.0)\n",
      "Row(features=SparseVector(262144, {1889: 7.4098, 3067: 7.589, 81535: 9.521, 92646: 5.0543, 106453: 8.7209, 108453: 5.9721, 136020: 5.0913, 175637: 7.1143, 235090: 9.5877}), label=0.0)\n",
      "Row(features=SparseVector(262144, {7695: 5.3318, 11910: 4.5874, 21872: 3.2963, 37521: 5.3421, 44133: 6.1187, 58267: 6.8682, 141063: 5.8936, 235090: 9.5877, 255396: 8.6923}), label=0.0)\n"
     ]
    }
   ],
   "source": [
    "for features_label in finalDF.select(\"features\", \"label\").take(3):\n",
    "    print(features_label)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Using ML libraries to classify data \n",
    "#Algorithm used:Naive Bayes\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing set \n",
    "(training, test) = finalDF.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes instance\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "# Use a pipeline to chain all transformers and estimators\n",
    "pipeline = Pipeline(stages=[nb])\n",
    "\n",
    "# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n",
    "# This will allow us to jointly choose parameters for all Pipeline stages.\n",
    "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(metricName=\"weightedPrecision\"),\n",
    "                          numFolds=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(label=0.0, title='\"Candy Crush Saga\" Maker Prices IPO', review_words=['\"candy', 'crush', 'saga\"', 'maker', 'prices', 'ipo'], filtered=['\"candy', 'crush', 'saga\"', 'maker', 'prices', 'ipo'], TF=SparseVector(262144, {19749: 1.0, 84079: 1.0, 87758: 1.0, 100824: 1.0, 137422: 1.0, 183237: 1.0}), features=SparseVector(262144, {19749: 10.4701, 84079: 11.5687, 87758: 5.9075, 100824: 5.5936, 137422: 6.7626, 183237: 4.6816}))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvModel = crossval.fit(training)\n",
    "\n",
    "# Make predictions on test documents. cvModel uses the best model found (lrModel).\n",
    "prediction = cvModel.transform(test)\n",
    "selected = prediction.select(\"title\", \"label\", \"probability\", \"prediction\").take(5)\n",
    "for row in selected:\n",
    "    print(row)\n",
    "\n",
    "# Evaluate result with accuracy and precision\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "\n",
    "weightedPrecision = evaluator.evaluate(prediction)\n",
    "weightedPrecision\n",
    "\n",
    "evaluator_accuracy=MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(prediction)\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
